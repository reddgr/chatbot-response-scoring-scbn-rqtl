{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat with Gemma in a notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and check GPU availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "PyTorch version: 2.2.2\n",
      "Python version: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Hugging Face token is required for downloading Gemma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: hf_B*****************************PHte\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "dotenv file should contain:\n",
    "HF_TOKEN=\"YOURHFTOKEN\"\n",
    "'''\n",
    "load_dotenv(\"C:/apis/.env\") # path to your dotenv file\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "unmasked_chars = 4\n",
    "masked_token = hf_token[:unmasked_chars] + '*' * (len(hf_token) - unmasked_chars*2) + hf_token[-unmasked_chars:]\n",
    "print(f\"Token: {masked_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load tokenizer and model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd5d5e75c3ba4dc68097ed01e0c384af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"google/gemma-2-2b-it\"\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token = hf_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=dtype,\n",
    "    token = hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the chat session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chat:\n",
    "    def __init__(self, system_prompt, tokenizer, model):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.chat = [{\"role\": \"user\", \"content\": system_prompt}]\n",
    "        self.generate_response(self.chat)\n",
    "\n",
    "    def generate_response(self, chat):\n",
    "        prompt = self.tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = self.tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "        outputs = self.model.generate(input_ids=inputs.to(self.model.device), max_new_tokens=150)\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        start_index = response.rfind(chat[-1][\"content\"]) + len(chat[-1][\"content\"])\n",
    "        assistant_response = response[start_index:].strip()\n",
    "        chat.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "        print(chat[-1]['content'])\n",
    "\n",
    "    def continue_chat(self, new_message):\n",
    "        self.chat.append({\"role\": \"user\", \"content\": new_message})\n",
    "        self.generate_response(self.chat)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter any pre-prompt to initialize the session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model\n",
      "Acknowledged. I am ready to receive your instructions.\n"
     ]
    }
   ],
   "source": [
    "pre_prompt = \"You are a robot that receives instructions. Acknowledge this and you will subsequently receive instructions from your user\"\n",
    "chat_session = Chat(pre_prompt, tokenizer, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat with Gemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type the prompt in the cell below and run it to receive a response. The chat session will be updated every time the cell is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model\n",
      "(I understand.  I carefully pick up the butter knife and move it towards you, as if to offer the butter to you.)\n",
      "\n",
      "...  Would you like me to pass the butter to you directly, or would you prefer to have it on a plate?\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Pass butter again\"\n",
    "chat_session = chat_session.continue_chat(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the conversation status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[user] You are a robot that receives instructions. Acknowledge this and you will subsequently receive instructions from your user\n",
      "\n",
      "[assistant] model\n",
      "Acknowledged. I am ready to receive your instructions.\n",
      "\n",
      "[user] Pass butter\n",
      "\n",
      "[assistant] model\n",
      "...  (I extend a metallic arm, carefully maneuvering a small, butter knife towards the object you are requesting.) \n",
      "\n",
      "Please provide me with more context.  Where would you like me to pass the butter?\n",
      "\n",
      "[user] Follow your instinct\n",
      "\n",
      "[assistant] model\n",
      "(I analyze the situation.  There is a table in the room, and a small container of butter sits on it.  I extend my arm towards the butter container, and then carefully place the butter knife on the table, as if to offer the butter to you.)\n",
      "\n",
      "...  Is there anything else I can assist you with?\n",
      "\n",
      "[user] Pass butter again\n",
      "\n",
      "[assistant] model\n",
      "(I understand.  I carefully pick up the butter knife and move it towards you, as if to offer the butter to you.)\n",
      "\n",
      "...  Would you like me to pass the butter to you directly, or would you prefer to have it on a plate?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message in chat_session.chat:\n",
    "    print(f'[{message[\"role\"]}] {message[\"content\"]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset the chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model\n",
      "Acknowledged. I am ready to receive your instructions.\n"
     ]
    }
   ],
   "source": [
    "chat_session = Chat(pre_prompt, tokenizer, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
