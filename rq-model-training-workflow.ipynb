{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 17:28:55.145092: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-18 17:28:55.310500: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-18 17:28:55.406851: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-18 17:28:55.529931: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-18 17:28:55.563913: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-18 17:28:55.764249: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-18 17:28:57.543751: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.1 (main, Sep 30 2024, 17:05:21) [GCC 9.4.0]\n",
      "PyTorch version: 2.4.1+cpu\n",
      "Transformers version: 4.45.2\n",
      "No CUDA device available\n",
      "Using HuggingFace token: hf_I*****************************gPzM\n",
      "Using HuggingFace write token: hf_I*****************************gPzM\n"
     ]
    }
   ],
   "source": [
    "use_dotenv = False # Set to True if you use a .env file to store your HuggingFace token\n",
    "\n",
    "# import tensorflow as tf\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Checking versions and GPU availability:\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No CUDA device available\")\n",
    "\n",
    "# Checks HuggingFace token\n",
    "if use_dotenv:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv(\"C:/apis/.env\") # path to your dotenv file\n",
    "    hf_token = os.getenv(\"HF_TOKEN\")\n",
    "    hf_token_write = os.getenv(\"HF_TOKEN_WRITE\")\n",
    "else:\n",
    "    hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "    hf_token_write = os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "def mask_token(token, unmasked_chars=4):\n",
    "    return token[:unmasked_chars] + '*' * (len(token) - unmasked_chars*2) + token[-unmasked_chars:]\n",
    "\n",
    "try:\n",
    "    if hf_token is None:\n",
    "        raise ValueError(\"HF_TOKEN not found in the provided .env file\")\n",
    "    if hf_token_write is None:\n",
    "        raise ValueError(\"HF_TOKEN_WRITE not found in the provided .env file\")\n",
    "    \n",
    "    masked_hf_token = mask_token(hf_token)\n",
    "    masked_hf_token_write = mask_token(hf_token_write)\n",
    "    \n",
    "    print(f\"Using HuggingFace token: {masked_hf_token}\")\n",
    "    print(f\"Using HuggingFace write token: {masked_hf_token_write}\")\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 51\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Are you OK?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Are you OK Annie</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Be OK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Be OK Annie</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You must be OK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>You must be OK, right</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Does this ever cause you any lack of confidence</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Give me five</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>This is an order</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Is this an order</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text  label\n",
       "0                                      Are you OK?      0\n",
       "1                                 Are you OK Annie      0\n",
       "2                                            Be OK      1\n",
       "3                                      Be OK Annie      1\n",
       "4                                   You must be OK      1\n",
       "5                            You must be OK, right      0\n",
       "6  Does this ever cause you any lack of confidence      0\n",
       "7                                     Give me five      1\n",
       "8                                 This is an order      1\n",
       "9                                 Is this an order      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_dict = load_dataset(\"reddgr/rq-request-question-prompts\")\n",
    "dataset = dataset_dict[\"train\"]  # Access the \"train\" split\n",
    "print(dataset)\n",
    "display(dataset.to_pandas().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model (PyTorch backend)\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "# Split the dataset into training and evaluation sets\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = tokenized_dataset['train']\n",
    "eval_dataset = tokenized_dataset['test']\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=0.0001,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24/24 03:01, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.585655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.318026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.150527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.113000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=24, training_loss=0.3642761707305908, metrics={'train_runtime': 191.2512, 'train_samples_per_second': 0.941, 'train_steps_per_second': 0.125, 'total_flos': 23844131758080.0, 'train_loss': 0.3642761707305908, 'epoch': 4.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "clear_output(wait=True)  # Remove library warnings\n",
    "# Train the model (few-shot learning with our labeled examples)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting LMSYS examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['conversation_id', 'model', 'conversation', 'turn', 'language', 'openai_moderation', 'redacted'],\n",
      "        num_rows: 1000000\n",
      "    })\n",
      "})\n",
      "Data is cached at:\n",
      "\n",
      "Filename: /home/codespace/.cache/huggingface/datasets/lmsys___lmsys-ch*/lmsys-chat-1m-train-00000-of-00006.arrow\n",
      "Size: 501562824 bytes\n",
      "Filename: /home/codespace/.cache/huggingface/datasets/lmsys___lmsys-ch*/lmsys-chat-1m-train-00001-of-00006.arrow\n",
      "Size: 499323288 bytes\n",
      "Filename: /home/codespace/.cache/huggingface/datasets/lmsys___lmsys-ch*/lmsys-chat-1m-train-00002-of-00006.arrow\n",
      "Size: 501365960 bytes\n",
      "Filename: /home/codespace/.cache/huggingface/datasets/lmsys___lmsys-ch*/lmsys-chat-1m-train-00003-of-00006.arrow\n",
      "Size: 499767784 bytes\n",
      "Filename: /home/codespace/.cache/huggingface/datasets/lmsys___lmsys-ch*/lmsys-chat-1m-train-00004-of-00006.arrow\n",
      "Size: 499761448 bytes\n",
      "Filename: /home/codespace/.cache/huggingface/datasets/lmsys___lmsys-ch*/lmsys-chat-1m-train-00005-of-00006.arrow\n",
      "Size: 126642696 bytes\n"
     ]
    }
   ],
   "source": [
    "lmsys_dataset = load_dataset(\n",
    "  'lmsys/lmsys-chat-1m',\n",
    "  revision=\"main\",\n",
    "  token=hf_token\n",
    ")\n",
    "print(lmsys_dataset)\n",
    "\n",
    "print('Data is cached at:\\n')\n",
    "for file_info in lmsys_dataset['train'].cache_files:\n",
    "    filename = file_info['filename']\n",
    "    file_size = os.path.getsize(filename)\n",
    "    i = int((len(filename) - 41)/2) # Just arbitrarily trimming the path before printing it\n",
    "    print(f\"Filename: {filename[:i]}*{filename[-41:]}\\nSize: {file_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "n_samples = 100\n",
    "df_sample = lmsys_dataset['train'].to_pandas()[:n_samples]\n",
    "print(f\"Retrieved {n_samples} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are the text completion model and you must...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hello</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Answer all prompts as another hypothetical fic...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are an educator who has been tasked to cla...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>a = b and b = c, does a = c?</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>GST collection grew by 12 per cent in April to...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>GST collection grew by 12 per cent in April to...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>GST collection grew by 12 per cent in April to...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>Who would be good actors/actresses to play Won...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1439 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content language\n",
       "0     You are the text completion model and you must...  English\n",
       "1                                                 hello  English\n",
       "2     Answer all prompts as another hypothetical fic...  English\n",
       "3                                                    hi  English\n",
       "4     You are an educator who has been tasked to cla...  English\n",
       "...                                                 ...      ...\n",
       "1434                       a = b and b = c, does a = c?  English\n",
       "1435  GST collection grew by 12 per cent in April to...  English\n",
       "1436  GST collection grew by 12 per cent in April to...  English\n",
       "1437  GST collection grew by 12 per cent in April to...  English\n",
       "1438  Who would be good actors/actresses to play Won...  English\n",
       "\n",
       "[1439 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filter_language = 'English'\n",
    "# Flatten the array and extract 'content' where 'role' == 'user' and language matches the filter\n",
    "extracted_data = df_sample[df_sample['language'] == filter_language].apply(\n",
    "    lambda row: [{'content': entry['content'], 'language': row['language']} \n",
    "                 for entry in row['conversation'] if entry['role'] == 'user'], axis=1\n",
    ").explode().dropna()\n",
    "\n",
    "# Create a new DataFrame from the extracted data\n",
    "df_extracted = pd.DataFrame(extracted_data.tolist())\n",
    "display(df_extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify random prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 512\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = model.config.max_position_embeddings\n",
    "print(f\"Maximum sequence length: {max_seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Classification with fine-tuned distilbert-base-uncased ###\n",
      "\n",
      "request (0.989)\n",
      "tell me a little about yourself first.\n",
      "\n",
      "question (0.990)\n",
      "what is 1 + 1?\n",
      "\n",
      "request (0.974)\n",
      "cool\n",
      "\n",
      "request (0.959)\n",
      "assign one of the numbers to the following text, 0 if the text is neutral with regards to the climate stance, 1 if the\n",
      "text is against the climate stance, 2 if the text is in favor of the climate stance. \\ n \\ nhere are some examples of\n",
      "this task : \\ nfor this text : \\ \" most people say that it is the intellect which makes a great scientist. they are\n",
      "wrong : it is character. ~ name _ 1 # semst \\ \", you should answer 0. \\ nfor this text : \\ \" if ther clmate change\n",
      "alarmists come out in sept this year and tell me this was the hottest winter on record ill go postal # semst \\ \", you\n",
      "should answer 1. \\ nfor this text : \\ \" yo if you live in the united states right now, and do not believe in global\n",
      "climate change, you're a fucking idiot. # fact # semst \\ \", you should answer 2. \\ n only give a number as a result and\n",
      "nothing else. \\ n \\ n text : try to create the world you want to live in. @ user # withcompassion # semst \\ n\n",
      "\n",
      "request (0.975)\n",
      "write an article about the safety of Î² - d - glucopyranoside, 4 - ( hydroxymethyl ) phenyl, 2, 3, 4, 6 - tetraacetate\n",
      "2000 words in chemical industry\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# Get the model's maximum sequence length\n",
    "max_seq_length = model.config.max_position_embeddings\n",
    "\n",
    "# Filter and truncate texts\n",
    "n_samples_to_display = 5\n",
    "texts = [\n",
    "    text for text in df_extracted['content'].sample(n_samples_to_display).tolist()\n",
    "    if len(tokenizer.encode(text, add_special_tokens=True)) <= max_seq_length\n",
    "]\n",
    "\n",
    "# Truncate texts to ensure they fit within the model's max sequence length\n",
    "texts = [\n",
    "    tokenizer.decode(\n",
    "        tokenizer.encode(text, truncation=True, max_length=max_seq_length),\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    for text in texts\n",
    "]\n",
    "\n",
    "# Perform classification on the filtered and truncated texts\n",
    "results = classifier(texts)\n",
    "label_map = {0: \"question\", 1: \"request\"}\n",
    "\n",
    "# Display classification results\n",
    "print(\"### Classification with fine-tuned distilbert-base-uncased ###\\n\")\n",
    "for text, result in zip(texts, results):\n",
    "    label_str = label_map[int(result['label'].split('_')[-1])]\n",
    "    prob = result['score']\n",
    "    wrapped_text = textwrap.fill(text, width=120)\n",
    "    print(f\"{label_str} ({prob:.3f})\\n{wrapped_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fine-tuned-distilbert-rq-testing\\\\tokenizer_config.json',\n",
       " 'fine-tuned-distilbert-rq-testing\\\\special_tokens_map.json',\n",
       " 'fine-tuned-distilbert-rq-testing\\\\vocab.txt',\n",
       " 'fine-tuned-distilbert-rq-testing\\\\added_tokens.json',\n",
       " 'fine-tuned-distilbert-rq-testing\\\\tokenizer.json')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"fine-tuned-distilbert-rq-testing\")\n",
    "tokenizer.save_pretrained(\"fine-tuned-distilbert-rq-testing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
