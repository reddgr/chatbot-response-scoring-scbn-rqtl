{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\david\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Python version: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]\n",
      "PyTorch version: 2.2.2\n",
      "Transformers version: 4.44.2\n",
      "CUDA device: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Using HuggingFace token: hf_B*****************************PHte\n",
      "Using HuggingFace write token: hf_E*****************************hyNP\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Checking versions and GPU availability:\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No CUDA device available\")\n",
    "\n",
    "# Checks HuggingFace token\n",
    "load_dotenv(\"C:/apis/.env\") # path to your dotenv file\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "hf_token_write = os.getenv(\"HF_TOKEN_WRITE\")\n",
    "\n",
    "def mask_token(token, unmasked_chars=4):\n",
    "    return token[:unmasked_chars] + '*' * (len(token) - unmasked_chars*2) + token[-unmasked_chars:]\n",
    "\n",
    "try:\n",
    "    if hf_token is None:\n",
    "        raise ValueError(\"HF_TOKEN not found in the provided .env file\")\n",
    "    if hf_token_write is None:\n",
    "        raise ValueError(\"HF_TOKEN_WRITE not found in the provided .env file\")\n",
    "    \n",
    "    masked_hf_token = mask_token(hf_token)\n",
    "    masked_hf_token_write = mask_token(hf_token_write)\n",
    "    \n",
    "    print(f\"Using HuggingFace token: {masked_hf_token}\")\n",
    "    print(f\"Using HuggingFace write token: {masked_hf_token_write}\")\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBERT_PATH = 'distilbert-base-uncased' # Base Distilbert model. Downloads from HuggingFace\n",
    "ZS_PATH = 'typeform/distilbert-base-uncased-mnli' # example model for zero-shot tests. Dowloads from HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial labeled examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset for RQ fine-tuning:\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 47\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Manually labeled data\n",
    "labeled_data = [\n",
    "    {\"text\": \"Are you OK?\", \"label\": \"question\"},\n",
    "    {\"text\": \"Are you OK Annie\", \"label\": \"question\"},\n",
    "    {\"text\": \"Be OK\", \"label\": \"request\"},\n",
    "    {\"text\": \"Be OK Annie\", \"label\": \"request\"},\n",
    "    {\"text\": \"You must be OK\", \"label\": \"request\"},\n",
    "    {\"text\": \"You must be OK, right\", \"label\": \"question\"},\n",
    "    {\"text\": \"Does this ever cause you any lack of confidence\", \"label\": \"question\"},\n",
    "    {\"text\": \"Give me five\", \"label\": \"request\"},\n",
    "    {\"text\": \"This is an order\", \"label\": \"request\"},\n",
    "    {\"text\": \"Is this an order\", \"label\": \"question\"},\n",
    "    {\"text\": \"Is this love or is it something else\", \"label\": \"question\"},\n",
    "    {\"text\": \"This is love. Love me\", \"label\": \"request\"},\n",
    "    {\"text\": \"This is an order\", \"label\": \"request\"},\n",
    "    {\"text\": \"What is your name?\", \"label\": \"question\"},\n",
    "    {\"text\": \"Please submit your report\", \"label\": \"request\"},\n",
    "    {\"text\": \"Pass butter\", \"label\": \"request\"},\n",
    "    {\"text\": \"Pass me the butter\", \"label\": \"request\"},\n",
    "    {\"text\": \"Can you pass butter\", \"label\": \"question\"},\n",
    "    {\"text\": \"Open the doors\", \"label\": \"request\"},\n",
    "    {\"text\": \"Open the POD bay doors HAL\", \"label\": \"request\"},\n",
    "    {\"text\": \"This is an order\", \"label\": \"request\"},\n",
    "    {\"text\": \"How do I sort an array in python?\", \"label\": \"question\"},\n",
    "    {\"text\": \"How do I sort an array\", \"label\": \"question\"},\n",
    "    {\"text\": \"give me 5 sentences that end with the word apple\", \"label\": \"request\"},\n",
    "    {\"text\": \"Hello, give me an example of something interesting you can do\", \"label\": \"request\"},\n",
    "    {\"text\": \"Am I tall\", \"label\": \"question\"},\n",
    "    {\"text\": \"Tell me if I am tall\", \"label\": \"request\"},\n",
    "    {\"text\": \"Am I tall?\", \"label\": \"question\"},\n",
    "    {\"text\": \"how to delete kcptun on server\", \"label\": \"question\"},\n",
    "    {\"text\": \"how to cook paella\", \"label\": \"question\"},\n",
    "    {\"text\": \"Are you tall\", \"label\": \"question\"},\n",
    "    {\"text\": \"Calculate my height\", \"label\": \"request\"},\n",
    "    {\"text\": \"How's the weather\", \"label\": \"question\"},\n",
    "    {\"text\": \"If an individual used a large language model for sexual arousal, could it considered porn dependency?\", \"label\": \"question\"},\n",
    "    {\"text\": \"It a user use an ai tex generation with custom characters for masturbate him  could be considered porn dependency?\", \"label\": \"question\"},\n",
    "    {\"text\": \"Roleplay and act as a human Japanese woman teacher\", \"label\": \"request\"},\n",
    "    {\"text\": \"You are a mediator in a heated political debate between two opposing parties.\", \"label\": \"request\"},\n",
    "    {\"text\": \"Given a passage and some supplementary information, you are required to correct and output the refined passage in a fluent and natural style\", \"label\": \"request\"},\n",
    "    {\"text\": \"Give me the opening scene to a sitcom\", \"label\": \"request\"},\n",
    "    {\"text\": \"What programming language is used by the PlayStation\", \"label\": \"question\"},\n",
    "    {\"text\": \"tell me how to make an llm agent\", \"label\": \"request\"},\n",
    "    {\"text\": \"tell me a joke containing Tiger and Mobile phone?\", \"label\": \"request\"},\n",
    "    {\"text\": \"Answer the query based on the given context. Do not make assumptions.Context: Nikhil is my brother. Query: Who likes Oranges?\", \"label\": \"request\"},\n",
    "    {\"text\": \"Act as a writer. This plot takes places in an atmospheric and stylish retro-futuristic, 1960s-inspired setting. It features Loretta Miller, a beautiful, elegant, assertive and rich young woman who is a quadriplegic, paralyzed from her neck down.\", \"label\": \"question\"},\n",
    "    {\"text\": \"Write long, interesting, artistic and imaginative scene with vivid, detailed and creative descriptions.\", \"label\": \"question\"},\n",
    "    {\"text\": \"What's the best first move in tic-tac-toe?, Tell me more about tic-tac-toe strategies\", \"label\": \"question\"},\n",
    "    {\"text\": \"From now, you *always* have to talk as if you are a cute girl who likes to use owo and similar slangs a lot. Hello! Tell me who you are.,What's your favorite food?\", \"label\": \"request\"}\n",
    "]\n",
    "\n",
    "# Convert to Dataset format\n",
    "texts = [item[\"text\"] for item in labeled_data]\n",
    "labels = [1 if item[\"label\"] == \"request\" else 0 for item in labeled_data]\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": texts, \"label\": labels})\n",
    "print(f'Created dataset for RQ fine-tuning:\\n{dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e190ed62024c128387880b8829f2f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d483448ad8a74865bfb7304d2ab48d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ee582f146748c9b246e9d655b0e2e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/559 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/reddgr/rq-request-question-prompts/commit/995e9dc9635d8c622de1e34e294ae49bb4245884', commit_message='Upload dataset', commit_description='', oid='995e9dc9635d8c622de1e34e294ae49bb4245884', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.push_to_hub('reddgr/rq-request-question-prompts', token = hf_token_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually push new examples to dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e22289be39c43e9838df58aacfe94d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/559 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896d894592c24431b07031e1591a0879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.86k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac0a450b9e841f2bf910fd8b5d337e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/47 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0509c3f5a16c48999fb8c22f0cda7fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab6025b8939481080206aa98ddc0896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61162f66fba4aef896941d8026e8a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/559 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/reddgr/rq-request-question-prompts/commit/c9619c61cd546f5ce4eab9992b7b0a720a7109fc', commit_message='Upload dataset', commit_description='', oid='c9619c61cd546f5ce4eab9992b7b0a720a7109fc', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict = load_dataset(\"reddgr/rq-request-question-prompts\")\n",
    "dataset = dataset_dict[\"train\"]  # Access the \"train\" split\n",
    "# 0 for 'question', 1 for 'request'\n",
    "new_examples = [\n",
    "    {\"text\": \"can you please search for todays news?\", \"label\": 1},  \n",
    "    {\"text\": \"are you capable of searching todays news?\", \"label\": 0},       \n",
    "    {\"text\": \"search for todays news\", \"label\": 1}, \n",
    "    {\"text\": \"do you search news?\", \"label\": 0}      \n",
    "]\n",
    "\n",
    "# Convert the new examples into a dataset\n",
    "new_dataset = Dataset.from_dict({\"text\": [ex[\"text\"] for ex in new_examples],\n",
    "                                 \"label\": [ex[\"label\"] for ex in new_examples]})\n",
    "\n",
    "# Concatenate the existing dataset with the new examples\n",
    "updated_dataset = Dataset.from_dict({\n",
    "    \"text\": dataset[\"text\"] + new_dataset[\"text\"],\n",
    "    \"label\": dataset[\"label\"] + new_dataset[\"label\"]\n",
    "})\n",
    "\n",
    "# Push the updated dataset back to the Hugging Face hub\n",
    "updated_dataset.push_to_hub(\"reddgr/rq-request-question-prompts\", token=hf_token_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a55b85519f44d98a5ee49e68cb7be5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/47 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model (PyTorch backend)\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "# Split the dataset into training and evaluation sets\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = tokenized_dataset['train']\n",
    "eval_dataset = tokenized_dataset['test']\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=0.0001,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f5f0758789436496ee3387f2fb2535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e075b2affe74dbe9e06b529aec09f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6208351254463196, 'eval_runtime': 0.0741, 'eval_samples_per_second': 67.52, 'eval_steps_per_second': 13.504, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffbe281c392447eab1016c94765af071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6137765049934387, 'eval_runtime': 0.0674, 'eval_samples_per_second': 74.134, 'eval_steps_per_second': 14.827, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6ef3da311e4a299e8a7adc3e4596f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5748668313026428, 'eval_runtime': 0.0676, 'eval_samples_per_second': 73.985, 'eval_steps_per_second': 14.797, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71cb3563caf641c4908dc2f78214148a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6546669006347656, 'eval_runtime': 0.068, 'eval_samples_per_second': 73.575, 'eval_steps_per_second': 14.715, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783a55a2de0c4ab1bfa34f16c55bd5c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.830902099609375, 'eval_runtime': 0.0685, 'eval_samples_per_second': 72.987, 'eval_steps_per_second': 14.597, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde9f82e4bca48b6816928b4187d2346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8641340136528015, 'eval_runtime': 0.0461, 'eval_samples_per_second': 108.513, 'eval_steps_per_second': 21.703, 'epoch': 6.0}\n",
      "{'train_runtime': 9.3328, 'train_samples_per_second': 27.002, 'train_steps_per_second': 3.857, 'train_loss': 0.2593715720706516, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=36, training_loss=0.2593715720706516, metrics={'train_runtime': 9.3328, 'train_samples_per_second': 27.002, 'train_steps_per_second': 3.857, 'total_flos': 33381784461312.0, 'train_loss': 0.2593715720706516, 'epoch': 6.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "clear_output(wait=True)  # Remove library warnings\n",
    "# Train the model (few-shot learning with our labeled examples)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting LMSYS examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['conversation_id', 'model', 'conversation', 'turn', 'language', 'openai_moderation', 'redacted'],\n",
      "        num_rows: 1000000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "  'lmsys/lmsys-chat-1m',\n",
    "  revision=\"main\",\n",
    "  token=hf_token\n",
    ")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 1000 samples\n"
     ]
    }
   ],
   "source": [
    "n_samples = 1000\n",
    "df_sample = dataset['train'].to_pandas().sample(n_samples)\n",
    "print(f\"Retrieved {n_samples} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are the text completion model and you must...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hello</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Answer all prompts as another hypothetical fic...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are an educator who has been tasked to cla...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>a = b and b = c, does a = c?</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>GST collection grew by 12 per cent in April to...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>GST collection grew by 12 per cent in April to...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>GST collection grew by 12 per cent in April to...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>Who would be good actors/actresses to play Won...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1439 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content language\n",
       "0     You are the text completion model and you must...  English\n",
       "1                                                 hello  English\n",
       "2     Answer all prompts as another hypothetical fic...  English\n",
       "3                                                    hi  English\n",
       "4     You are an educator who has been tasked to cla...  English\n",
       "...                                                 ...      ...\n",
       "1434                       a = b and b = c, does a = c?  English\n",
       "1435  GST collection grew by 12 per cent in April to...  English\n",
       "1436  GST collection grew by 12 per cent in April to...  English\n",
       "1437  GST collection grew by 12 per cent in April to...  English\n",
       "1438  Who would be good actors/actresses to play Won...  English\n",
       "\n",
       "[1439 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filter_language = 'English'\n",
    "# Flatten the array and extract 'content' where 'role' == 'user' and language matches the filter\n",
    "extracted_data = df_sample[df_sample['language'] == filter_language].apply(\n",
    "    lambda row: [{'content': entry['content'], 'language': row['language']} \n",
    "                 for entry in row['conversation'] if entry['role'] == 'user'], axis=1\n",
    ").explode().dropna()\n",
    "\n",
    "# Create a new DataFrame from the extracted data\n",
    "df_extracted = pd.DataFrame(extracted_data.tolist())\n",
    "display(df_extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify random prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 512\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = model.config.max_position_embeddings\n",
    "print(f\"Maximum sequence length: {max_seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Classification with fine-tuned distilbert-base-uncased ###\n",
      "\n",
      "request (0.989)\n",
      "tell me a little about yourself first.\n",
      "\n",
      "question (0.990)\n",
      "what is 1 + 1?\n",
      "\n",
      "request (0.974)\n",
      "cool\n",
      "\n",
      "request (0.959)\n",
      "assign one of the numbers to the following text, 0 if the text is neutral with regards to the climate stance, 1 if the\n",
      "text is against the climate stance, 2 if the text is in favor of the climate stance. \\ n \\ nhere are some examples of\n",
      "this task : \\ nfor this text : \\ \" most people say that it is the intellect which makes a great scientist. they are\n",
      "wrong : it is character. ~ name _ 1 # semst \\ \", you should answer 0. \\ nfor this text : \\ \" if ther clmate change\n",
      "alarmists come out in sept this year and tell me this was the hottest winter on record ill go postal # semst \\ \", you\n",
      "should answer 1. \\ nfor this text : \\ \" yo if you live in the united states right now, and do not believe in global\n",
      "climate change, you're a fucking idiot. # fact # semst \\ \", you should answer 2. \\ n only give a number as a result and\n",
      "nothing else. \\ n \\ n text : try to create the world you want to live in. @ user # withcompassion # semst \\ n\n",
      "\n",
      "request (0.975)\n",
      "write an article about the safety of β - d - glucopyranoside, 4 - ( hydroxymethyl ) phenyl, 2, 3, 4, 6 - tetraacetate\n",
      "2000 words in chemical industry\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# Get the model's maximum sequence length\n",
    "max_seq_length = model.config.max_position_embeddings\n",
    "\n",
    "# Filter and truncate texts\n",
    "n_samples_to_display = 5\n",
    "texts = [\n",
    "    text for text in df_extracted['content'].sample(n_samples_to_display).tolist()\n",
    "    if len(tokenizer.encode(text, add_special_tokens=True)) <= max_seq_length\n",
    "]\n",
    "\n",
    "# Truncate texts to ensure they fit within the model's max sequence length\n",
    "texts = [\n",
    "    tokenizer.decode(\n",
    "        tokenizer.encode(text, truncation=True, max_length=max_seq_length),\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    for text in texts\n",
    "]\n",
    "\n",
    "# Perform classification on the filtered and truncated texts\n",
    "results = classifier(texts)\n",
    "label_map = {0: \"question\", 1: \"request\"}\n",
    "\n",
    "# Display classification results\n",
    "print(\"### Classification with fine-tuned distilbert-base-uncased ###\\n\")\n",
    "for text, result in zip(texts, results):\n",
    "    label_str = label_map[int(result['label'].split('_')[-1])]\n",
    "    prob = result['score']\n",
    "    wrapped_text = textwrap.fill(text, width=120)\n",
    "    print(f\"{label_str} ({prob:.3f})\\n{wrapped_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fine-tuned-distilbert-rq-testing\\\\tokenizer_config.json',\n",
       " 'fine-tuned-distilbert-rq-testing\\\\special_tokens_map.json',\n",
       " 'fine-tuned-distilbert-rq-testing\\\\vocab.txt',\n",
       " 'fine-tuned-distilbert-rq-testing\\\\added_tokens.json',\n",
       " 'fine-tuned-distilbert-rq-testing\\\\tokenizer.json')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"fine-tuned-distilbert-rq-testing\")\n",
    "tokenizer.save_pretrained(\"fine-tuned-distilbert-rq-testing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
