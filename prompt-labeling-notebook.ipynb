{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ (Request vs Question) and TL (Test vs Learn) labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to manually label prompts and add them to the [reddgr/rq-request-question-prompts](https://huggingface.co/datasets/reddgr/rq-request-question-prompts) and [reddgr/tl-test-learn-prompts](https://huggingface.co/datasets/reddgr/tl-test-learn-prompts) datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "colab = False\n",
    "if colab:\n",
    "    !pip install datasets\n",
    "    !git clone https://github.com/reddgr/chatbot-response-scoring-scbn-rqtl\n",
    "    import os\n",
    "    os.system(\"mv chatbot-response-scoring-scbn-rqtl scbn_rqtl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved HuggingFace token(s) from .env file\n",
      "Using HuggingFace token: hf_M*****************************IASJ\n",
      "Using HuggingFace write token: hf_u*****************************Xipx\n"
     ]
    }
   ],
   "source": [
    "colab = False\n",
    "use_dotenv = True\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from IPython.display import clear_output\n",
    "\n",
    "if colab:\n",
    "  from scbn_rqtl import lmsys_dataset_handler as lmsys\n",
    "  from scbn_rqtl import labeling_widget\n",
    "else:\n",
    "  import lmsys_dataset_handler as lmsys\n",
    "  import labeling_widget\n",
    "\n",
    "# Checks HuggingFace token\n",
    "if use_dotenv:\n",
    "    print(\"Retrieved HuggingFace token(s) from .env file\")\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv(\"C:/apis/.env\") # path to your dotenv file\n",
    "    hf_token = os.getenv(\"HF_TOKEN\")\n",
    "    hf_token_write = os.getenv(\"HF_TOKEN_WRITE\") # Only used for updating the Reddgr dataset (privileges needed)\n",
    "elif colab:\n",
    "    from google.colab import userdata\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "    hf_token_write = userdata.get('HF_TOKEN_WRITE')\n",
    "else:\n",
    "    print(\"Retrieved HuggingFace token(s) from environment variables\")\n",
    "    hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "    hf_token_write = os.environ.get(\"HF_TOKEN\") # You don't have a token with write permission unless authorized, so you can just use the same token in these two variables\n",
    "\n",
    "def mask_token(token, unmasked_chars=4):\n",
    "    return token[:unmasked_chars] + '*' * (len(token) - unmasked_chars*2) + token[-unmasked_chars:]\n",
    "\n",
    "if hf_token is None:\n",
    "    raise ValueError(\"HF_TOKEN not found in the provided .env file\" if use_dotenv else \"HF_TOKEN not found in the environment variables\")\n",
    "if hf_token_write is None:\n",
    "    raise ValueError(\"HF_TOKEN_WRITE not found in the provided .env file\" if use_dotenv else \"HF_TOKEN_WRITE not found in the environment variables\")\n",
    "\n",
    "masked_hf_token = mask_token(hf_token)\n",
    "masked_hf_token_write = mask_token(hf_token_write)\n",
    "\n",
    "print(f\"Using HuggingFace token: {masked_hf_token}\")\n",
    "print(f\"Using HuggingFace write token: {masked_hf_token_write}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 300 records\n",
      "Extracted data from lmsys/lmsys-chat-1m. Prompt sample:\n",
      "\n",
      "Describe the paper: \"Attention is all you need\"?\n"
     ]
    }
   ],
   "source": [
    "N_SAMPLES = 300 # Number of full conversations to extract from the dataset\n",
    "MAX_CHAR_LENGTH = 500 # Maximum character length of the prompts to be labeled\n",
    "\n",
    "lmsys_chat_1m = lmsys.LMSYSChat1MHandler(hf_token, streaming=False, verbose=False)\n",
    "df_sample = lmsys_chat_1m.extract_df_sample(N_SAMPLES)\n",
    "df_prompts = lmsys_chat_1m.extract_prompts(filter_language=['English'], max_char_length=MAX_CHAR_LENGTH)\n",
    "prompt_sample = lmsys_chat_1m.extract_prompt_sample()\n",
    "print(\"Extracted data from lmsys/lmsys-chat-1m. Prompt sample:\\n\")\n",
    "print(prompt_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a921b3a7574dcbafc8f1315fbaec3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Button(button_style='success', description='CORRECT', style=ButtonStyle()), Buttâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LABELING_DATASET = \"RQ\" # \"TL\" for test-learn, \"RQ\" for request-question\n",
    "\n",
    "if LABELING_DATASET == \"TL\":\n",
    "    print(\"Labeling test-learn prompts\")\n",
    "    model_path = \"reddgr/tl-test-learn-prompt-classifier\"\n",
    "    label_map = {0: \"learn\", 1: \"test\"}\n",
    "    dataset_name = \"reddgr/tl-test-learn-prompts\"\n",
    "elif LABELING_DATASET == \"RQ\":\n",
    "    print(\"Labeling request-question prompts\")\n",
    "    model_path = \"reddgr/rq-request-question-prompt-classifier\"\n",
    "    label_map = {0: \"question\", 1: \"request\"}\n",
    "    dataset_name = \"reddgr/rq-request-question-prompts\"\n",
    "else:\n",
    "    raise ValueError(f\"Invalid labeling dataset: {LABELING_DATASET}\")\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "classifier = pipeline(\"text-classification\", model=model_path, tokenizer=model_path, device=device)\n",
    "clear_output(wait=True)\n",
    "tl_labeling_widget = labeling_widget.LabelingWidget()\n",
    "# Start the manual labeling process\n",
    "\n",
    "df_prompts.rename(columns={'prompt': 'text'}, inplace=True)\n",
    "tl_labeling_widget.manual_labeling(df_prompts, classifier, label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pushing labeled data to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae799da77e54da1899b509208b60764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/1.31k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec0951db0d914ebe9b013d970114dbc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.95k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f327cb860d48e99cc84d62e7205102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/8.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c594b7c7ed44e70a54f3ea5a3c5f45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/51 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "440645676c794b69926031f2f24ddbf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/143 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0aca4e53954a91a7fc118cb6f07ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1887fea7546648c4b25fb0a3f3cf8680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e032b38db84b9e939cabb6c9ae74f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22038bf1d1ef4c6fa1081de60387c2eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0800c828e34ab9873b53fc076bab6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.31k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\david\\.cache\\huggingface\\hub\\datasets--reddgr--rq-request-question-prompts. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pushed 2 records to reddgr/rq-request-question-prompts test split.\n"
     ]
    }
   ],
   "source": [
    "tl_labeling_widget.update_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    split_name=\"test\", # Choose either test or train split\n",
    "    hf_token=hf_token_write\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
