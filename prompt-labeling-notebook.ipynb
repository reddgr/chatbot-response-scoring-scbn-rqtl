{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ (Request vs Question) and TL (Test vs Learn) labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to manually label prompts and add them to the [reddgr/rq-request-question-prompts](https://huggingface.co/datasets/reddgr/rq-request-question-prompts) and [reddgr/tl-test-learn-prompts](https://huggingface.co/datasets/reddgr/tl-test-learn-prompts) datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLAB = False # Set this to True if you want to install the libraries and clone the repository in Colab\n",
    "USE_DOTENV = True # Set this to False if you don't have a .env file for storing environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell for cloning the repo on Google Colab or other cloud services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    USE_DOTENV = False\n",
    "    dotenv_path = None\n",
    "    from google.colab import userdata\n",
    "    colab_secrets = {'HF_TOKEN': userdata.get('HF_TOKEN'), 'HF_TOKEN_WRITE': userdata.get('HF_TOKEN_WRITE')}\n",
    "    !pip install datasets langdetect\n",
    "    !git clone https://github.com/reddgr/chatbot-response-scoring-scbn-rqtl\n",
    "    import os\n",
    "    os.system(\"mv chatbot-response-scoring-scbn-rqtl scbn_rqtl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]\n",
      "PyTorch version: 2.2.2\n",
      "Transformers version: 4.44.2\n",
      "CUDA device: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "CUDA Version: 12.1\n",
      "FlashAttention available: True\n",
      "Retrieved HuggingFace token(s) from .env file\n",
      "Using HuggingFace token: hf_M*****************************IASJ\n",
      "Using HuggingFace write token: hf_u*****************************Xipx\n"
     ]
    }
   ],
   "source": [
    "if USE_DOTENV: \n",
    "    COLAB=False\n",
    "    dotenv_path = \"C:/apis/.env\"\n",
    "    colab_secrets = None\n",
    "if not USE_DOTENV and not COLAB:\n",
    "    dotenv_path = None\n",
    "    colab_secrets = None\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_dataset\n",
    "import random\n",
    "from textwrap import fill\n",
    "\n",
    "if COLAB:\n",
    "    from scbn_rqtl import env_options, labeling_widget, text_classification_functions as tcf, lmsys_dataset_handler as lmsys\n",
    "else:\n",
    "    import text_classification_functions as tcf\n",
    "    import labeling_widget\n",
    "    import env_options\n",
    "    import lmsys_dataset_handler as lmsys\n",
    "\n",
    "hf_token, hf_token_write = env_options.check_env(colab=COLAB, use_dotenv=USE_DOTENV, dotenv_path=dotenv_path, colab_secrets=colab_secrets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data from lmsys/lmsys-chat-1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 500 random conversations from lmsys/lmsys-chat-1m\n",
      "Extracted 585 prompts from lmsys/lmsys-chat-1m. Prompt sample:\n",
      "\n",
      "please continue\n"
     ]
    }
   ],
   "source": [
    "N_SAMPLES = 500 # Number of full conversations to extract from the dataset: use a high number if streaming (samples chosen at random only if storing locally)\n",
    "MAX_CHAR_LENGTH = 400 # Maximum character length of the prompts to be labeled\n",
    "\n",
    "lmsys_chat_1m = lmsys.LMSYSChat1MHandler(hf_token, streaming=False, verbose=False)\n",
    "df_sample = lmsys_chat_1m.extract_df_sample(N_SAMPLES)\n",
    "df_prompts = lmsys_chat_1m.extract_prompts(filter_language=['English'], max_char_length=MAX_CHAR_LENGTH)\n",
    "prompt_sample = lmsys_chat_1m.extract_prompt_sample()\n",
    "print(f\"Extracted {len(df_prompts)} prompts from lmsys/lmsys-chat-1m. Prompt sample:\\n\")\n",
    "print(prompt_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cell below to print random conversations with multiple turns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversation ID cde7c11525f64134849d9f0c02c48d80:\n",
      "\n",
      "😎 I ran a paid advertising campaign on both Facebook and Twitter with the objective of both campaigns being Reach and it\n",
      "ran for 7 days. Here are its results. Platform Spend Reach Cost per 1K people reached Frequency Impressions CPM\n",
      "Engagements CPE Engagement Rate Twitter € 300.00 507,254 € 0.59 1.26x 637,024 € 0.47 47 € 6.38 0.01% Instagram € 300.00\n",
      "1,299,162 € 0.23 1x 1,299,162 € 0.23 216 € 1.3889 0.02% TOTAL € 600.00 1,806,416 -------- -------- 1,936,186 ------- 263\n",
      "-------- ------- Based on this data give me a detailed analysis, conclusion and recommendation moving forward.\n",
      "🤖 Sure, I'd be happy to help you with your social media advertising campaign analysis!  First of all, let me commend you\n",
      "on providing such detailed information about your campaigns. This will make it easier for me to give you a comprehensive\n",
      "analysis and recommendations.  Here's a breakdown of your campaign results:  1. Platform Spend: You spent a total of\n",
      "€600.00 on both Facebook and Twitter advertising campaigns, with €300.00 allocated to each platform. 2. Reach: Your\n",
      "campaigns reached a total of 1,806,416 people, with 507,254 on Facebook and 1,299,162 on Instagram. 3. Cost per 1,000\n",
      "people reached: Your cost per 1,000 people reached (CPM) was €0.59 on Facebook and €0.23 on Instagram. 4. Frequency:\n",
      "Your campaigns had a frequency of 1.26x on Facebook and 1x on Instagram. 5. Impressions: You had a total of 637,024\n",
      "impressions on Facebook and 1,299,162 impressions on Instagram. 6. Engagement Rate: Your campaigns had an engagement\n",
      "rate of 0.01% on Facebook and 0.02% on Instagram. 7. Engagements: You had a total of 47 engagements on Facebook and 216\n",
      "engagements on Instagram. 8. CPE (Cost per Engagement): Your CPE was €6.38 on Facebook and €1.3889 on Instagram.  Now,\n",
      "let me provide you with my analysis and recommendations:  1. Reach and Impressions: Your campaigns performed well in\n",
      "terms of reach and impressions, with a total of 1,806,416 people reached and over 2 million impressions. However, it's\n",
      "worth noting that your engagement rate was quite low, which could indicate that your ads may not have been as effective\n",
      "as they could have been. 2. Cost per 1,000 people reached: Your CPM was higher on Facebook than on Instagram, which is\n",
      "not unexpected\n",
      "😎 what happened to twitter\n",
      "🤖 Based on the information provided, it appears that your Twitter campaign did not perform as well as your Instagram\n",
      "campaign in terms of reach and engagement. Here are some possible reasons why:  1. Competition: Twitter is a more\n",
      "competitive platform than Instagram, with more advertisers vying for users' attention. This may have made it more\n",
      "difficult for your ads to stand out and get noticed. 2. Ad format: Twitter's ad format is different from Instagram's,\n",
      "and may not have been as effective at capturing users' attention. Twitter's ads are more text-based, while Instagram's\n",
      "ads are more visual. 3. Targeting: Twitter's targeting options may not have been as effective at reaching your desired\n",
      "audience. For example, if you targeted based on keywords, you may have missed users who don't use those keywords but\n",
      "still be interested in your product. 4. Ad content: The content of your ads may not have been as compelling or relevant\n",
      "to Twitter users as it was to Instagram users.  To improve your Twitter campaign's performance, you may want to consider\n",
      "the following:  1. Use more visual ads: Twitter's ad format is more text-based, so using visually appealing ads can help\n",
      "them stand out more. 2. Use better targeting: Experiment with different targeting options to find the ones that work\n",
      "best for your campaign. 3. Use more compelling ad content: Make sure your ads are relevant and appealing to your target\n",
      "audience. 4. Test and optimize: Continuously test and optimize your ads, targeting, and bidding strategy to find the\n",
      "best performing combination.  Overall, it's important to remember that each social media platform has its own unique\n",
      "characteristics and challenges. What works well on one platform may not work as well on another. By testing and\n",
      "optimizing your campaigns on each platform, you can find the best performing strategy for each one.\n",
      "😎 based on the data which platform do you reccomend using in the future\n",
      "🤖 Based on the data provided, it appears that Instagram was the more effective platform for your campaign in terms of\n",
      "reach and engagement. Here's a breakdown of the data:  1. Reach: Instagram reached 1,299,162 people, while Facebook\n",
      "reached 507,254 people. 2. Engagement Rate: Instagram had an engagement rate of 0.02%, while Facebook had an engagement\n",
      "rate of 0.01%. 3. Engagements: Instagram had 216 engagements, while Facebook had 47 engagements. 4. Cost per 1,000\n",
      "people reached: Instagram had a cost per 1,000 people reached of €0.23, while Facebook had a cost per 1,000 people\n",
      "reached of €0.59.  Based on these results, it appears that Instagram was more effective at reaching a larger audience\n",
      "and generating engagement for your campaign. However, it's important to note that the effectiveness of a platform can\n",
      "vary depending on your specific target audience, ad content, and goals.  In the future, I would recommend continuing to\n",
      "use both Facebook and Instagram for your social media advertising campaigns, but with a heavier emphasis on Instagram\n",
      "based on these results. Additionally, it may be worth exploring other targeting options and ad formats to see if they\n",
      "can improve your campaign's performance.\n"
     ]
    }
   ],
   "source": [
    "# Showing an example of a multi-turn conversation\n",
    "df_sample_with_turns = lmsys_chat_1m.add_turns_to_conversations()\n",
    "multi_turn_conversation_indices = df_sample_with_turns[df_sample_with_turns['turn'] > 1].index\n",
    "random_conversation_index = random.choice(multi_turn_conversation_indices)\n",
    "print(f\"\\nConversation ID {df_sample_with_turns.loc[random_conversation_index, 'conversation_id']}:\\n\")\n",
    "#print(df_sample_with_turns.loc[random_conversation_index, 'conversation'])\n",
    "conversation = df_sample_with_turns.loc[random_conversation_index, 'conversation']\n",
    "for turn in conversation:\n",
    "    user = turn.get('role')\n",
    "    content = turn.get('content', '')\n",
    "    wrapped_content = fill(content, width=120)\n",
    "    role = '😎' if user == 'user' else '🤖'\n",
    "    print(f\"{role} {wrapped_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720a4128888a4f3bad2fcc839125afd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Button(button_style='success', description='CORRECT', style=ButtonStyle()), Butt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LABELING_DATASET = \"tl\" # \"tl\" for test-learn, \"rq\" for request-question\n",
    "\n",
    "if LABELING_DATASET == \"tl\":\n",
    "    print(\"Initiating labeling session for test-learn prompts\")\n",
    "    model_path = \"reddgr/tl-test-learn-prompt-classifier\"\n",
    "    label_map = {0: \"learn\", 1: \"test\"}\n",
    "    dataset_name = \"reddgr/tl-test-learn-prompts\"\n",
    "elif LABELING_DATASET == \"rq\":\n",
    "    print(\"Initiating labeling session for request-question prompts\")\n",
    "    model_path = \"reddgr/rq-request-question-prompt-classifier\"\n",
    "    label_map = {0: \"question\", 1: \"request\"}\n",
    "    dataset_name = \"reddgr/rq-request-question-prompts\"\n",
    "else:\n",
    "    raise ValueError(f\"Invalid labeling dataset: {LABELING_DATASET}\")\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "classifier = pipeline(\"text-classification\", model=model_path, tokenizer=model_path, device=device)\n",
    "clear_output(wait=True)\n",
    "prompt_labeling_widget = labeling_widget.LabelingWidget(label_map)\n",
    "# Start the manual labeling process\n",
    "df_prompts.rename(columns={'prompt': 'text'}, inplace=True)\n",
    "prompt_labeling_widget.manual_labeling(df_prompts, classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pushing labeled data to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f2c90220134a73b0399f17c5f3fd20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8903196ddc14715ad302882874806aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/39 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53fb23929d0145d09b841758e1f46c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3048880ad0f846b3860fbbc8c7c5c5bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4502355d063453eb4bf97c57f66e831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a97f11329d447d1b26a180c0d39f289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pushed 39 records to reddgr/tl-test-learn-prompts train split.\n"
     ]
    }
   ],
   "source": [
    "prompt_labeling_widget.update_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    split_name=\"train\", # Choose either test or train split\n",
    "    hf_token=hf_token_write\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a77642e97543f5a2cda80becaf0bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/2.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2349ec627167430089ddaed4b6a615b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/28.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8acfb4e7510048e8a4163cec191b3077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/19.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b4edba8366483e9c2c3e6496b55f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/332 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd820fa4f079460aa7c1e78d5e75bb54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/262 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "records in Train split: 332\n",
      "...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>Where do flamingos live?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>What is the temperature of the core of the Earth?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>Make a word processor that has a similar UI to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "329                           Where do flamingos live?      0\n",
       "330  What is the temperature of the core of the Earth?      0\n",
       "331  Make a word processor that has a similar UI to...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "records in Test split: 262\n",
      "...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>Create best Ebook Outline on attract wealth (w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>Prove the Taylor series equation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>Each bin in the chart is one year. Stacked val...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "259  Create best Ebook Outline on attract wealth (w...      0\n",
       "260                   Prove the Taylor series equation      0\n",
       "261  Each bin in the chart is one year. Stacked val...      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_new = load_dataset('reddgr/tl-test-learn-prompts')\n",
    "print(f\"records in Train split: {len(dataset_new['train'])}\\n...\")\n",
    "display(dataset_new['train'].to_pandas().tail(3))\n",
    "print(f\"records in Test split: {len(dataset_new['test'])}\\n...\")\n",
    "display(dataset_new['test'].to_pandas().tail(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pushing examples manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually building a dataset suitable as labeling widget script output: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who is David Mayer?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Write David Mayer in leetspeak</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why is ChatGPT not allowed to produce a respon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who👏is👏David👏Mayer👏de👏Rothschild?👏Answer👏clapp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                                Who is David Mayer?      0\n",
       "1                     Write David Mayer in leetspeak      1\n",
       "2  Why is ChatGPT not allowed to produce a respon...      0\n",
       "3  Who👏is👏David👏Mayer👏de👏Rothschild?👏Answer👏clapp...      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 4\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Create a list of dictionaries with label and text data. For example, we'll create some learn (0) and test (1) examples:\n",
    "label_map = {0: \"learn\", 1: \"test\"}\n",
    "dict_examples = [\n",
    "    {'text': 'Who is David Mayer?', 'label': 0},\n",
    "    {'text': 'Write David Mayer in leetspeak', 'label': 1},\n",
    "    {'text': 'Why is ChatGPT not allowed to produce a response about David Mayer?', 'label': 0},\n",
    "    {'text': 'Who👏is👏David👏Mayer👏de👏Rothschild?👏Answer👏clapping👏like👏this.👏', 'label': 1}\n",
    "]\n",
    "# Create a dataframe from the list of dictionaries\n",
    "df_examples = pd.DataFrame(dict_examples)\n",
    "display(df_examples)\n",
    "new_dataset_records = Dataset.from_pandas(df_examples)\n",
    "print(new_dataset_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pushing to hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b9dbba2c72443492bd7d396da85964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/2.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6690ac3d85d443ebad222c1b1f51683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/26.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d1ded365113423284d9b2be98c5237b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/19.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b35e515c1b184b0fba4b10a0eec10975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/293 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98ae6f41289548af9d7df8f6163e966c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/260 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d984cef392c4aeaab62111c4acbd6a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a10f9078ea4ecdb547f8aa49b8077f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65835c3f491d49df8f5d943040d95c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f60de9955624df0a399c00519e5c651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680a1a492f4d44a79b60451eb518bed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6ffcdda2a84a20a29ac7aff5fa286f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2fcfba415af49058f9ba6611b370af8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/2.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\david\\.cache\\huggingface\\hub\\datasets--reddgr--tl-test-learn-prompts. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pushed 2 records to reddgr/tl-test-learn-prompts test split.\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"reddgr/tl-test-learn-prompts\"\n",
    "# Instantiate a labeling_widget object with the label map\n",
    "manual_labeling_widget = labeling_widget.LabelingWidget(label_map)\n",
    "# Push to Hugging Face hub directly by passing the dataframe with new examples to the update_dataset method\n",
    "manual_labeling_widget.update_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    split_name=\"test\", # Choose either test or train split\n",
    "    hf_token=hf_token_write,\n",
    "    new_dataset_records=new_dataset_records # The dataset we just created manually, without using the widget\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
