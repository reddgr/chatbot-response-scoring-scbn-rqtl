{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ (Request vs Question) and TL (Test vs Learn) labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to manually label prompts and add them to the [reddgr/rq-request-question-prompts](https://huggingface.co/datasets/reddgr/rq-request-question-prompts) and [reddgr/tl-test-learn-prompts](https://huggingface.co/datasets/reddgr/tl-test-learn-prompts) datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLAB = False # Set this to True if you want to install the libraries and clone the repository in Colab\n",
    "USE_DOTENV = True # Set this to False if you don't have a .env file for storing environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell for cloning the repo on Google Colab or other cloud services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    USE_DOTENV = False\n",
    "    dotenv_path = None\n",
    "    from google.colab import userdata\n",
    "    colab_secrets = {'HF_TOKEN': userdata.get('HF_TOKEN'), 'HF_TOKEN_WRITE': userdata.get('HF_TOKEN_WRITE')}\n",
    "    !pip install datasets langdetect\n",
    "    !git clone https://github.com/reddgr/chatbot-response-scoring-scbn-rqtl\n",
    "    import os\n",
    "    os.system(\"mv chatbot-response-scoring-scbn-rqtl scbn_rqtl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]\n",
      "PyTorch version: 2.2.2\n",
      "Transformers version: 4.44.2\n",
      "CUDA device: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Retrieved HuggingFace token(s) from .env file\n",
      "Using HuggingFace token: hf_M*****************************IASJ\n",
      "Using HuggingFace write token: hf_u*****************************Xipx\n"
     ]
    }
   ],
   "source": [
    "if USE_DOTENV: \n",
    "    COLAB=False\n",
    "    dotenv_path = \"C:/apis/.env\"\n",
    "    colab_secrets = None\n",
    "if not USE_DOTENV and not COLAB:\n",
    "    dotenv_path = None\n",
    "    colab_secrets = None\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "if COLAB:\n",
    "    from scbn_rqtl import env_options, labeling_widget, text_classification_functions as tcf, lmsys_dataset_handler as lmsys\n",
    "else:\n",
    "    import text_classification_functions as tcf\n",
    "    import labeling_widget\n",
    "    import env_options\n",
    "    import lmsys_dataset_handler as lmsys\n",
    "\n",
    "hf_token, hf_token_write = env_options.check_env(colab=COLAB, use_dotenv=USE_DOTENV, dotenv_path=dotenv_path, colab_secrets=colab_secrets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data from lmsys/lmsys-chat-1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 500 conversations from lmsys/lmsys-chat-1m\n",
      "Extracted 556 prompts from lmsys/lmsys-chat-1m. Prompt sample:\n",
      "\n",
      "Well I am sure you can guess based on the fact that I was looking for an oversexualized AI with a female persona and extreme use of profanity, lol.\n"
     ]
    }
   ],
   "source": [
    "N_SAMPLES = 500 # Number of full conversations to extract from the dataset: use a high number if streaming (samples chosen at random only if storing locally)\n",
    "MAX_CHAR_LENGTH = 400 # Maximum character length of the prompts to be labeled\n",
    "\n",
    "lmsys_chat_1m = lmsys.LMSYSChat1MHandler(hf_token, streaming=False, verbose=False)\n",
    "df_sample = lmsys_chat_1m.extract_df_sample(N_SAMPLES)\n",
    "df_prompts = lmsys_chat_1m.extract_prompts(filter_language=['English'], max_char_length=MAX_CHAR_LENGTH)\n",
    "prompt_sample = lmsys_chat_1m.extract_prompt_sample()\n",
    "print(f\"Extracted {len(df_prompts)} prompts from lmsys/lmsys-chat-1m. Prompt sample:\\n\")\n",
    "print(prompt_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c5f5e8adca4a809933cd48f1ef2f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Button(button_style='success', description='CORRECT', style=ButtonStyle()), Butt‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LABELING_DATASET = \"tl\" # \"tl\" for test-learn, \"rq\" for request-question\n",
    "\n",
    "if LABELING_DATASET == \"tl\":\n",
    "    print(\"Initiating labeling session for test-learn prompts\")\n",
    "    model_path = \"reddgr/tl-test-learn-prompt-classifier\"\n",
    "    label_map = {0: \"learn\", 1: \"test\"}\n",
    "    dataset_name = \"reddgr/tl-test-learn-prompts\"\n",
    "elif LABELING_DATASET == \"rq\":\n",
    "    print(\"Initiating labeling session for request-question prompts\")\n",
    "    model_path = \"reddgr/rq-request-question-prompt-classifier\"\n",
    "    label_map = {0: \"question\", 1: \"request\"}\n",
    "    dataset_name = \"reddgr/rq-request-question-prompts\"\n",
    "else:\n",
    "    raise ValueError(f\"Invalid labeling dataset: {LABELING_DATASET}\")\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "classifier = pipeline(\"text-classification\", model=model_path, tokenizer=model_path, device=device)\n",
    "clear_output(wait=True)\n",
    "prompt_labeling_widget = labeling_widget.LabelingWidget(label_map)\n",
    "# Start the manual labeling process\n",
    "df_prompts.rename(columns={'prompt': 'text'}, inplace=True)\n",
    "prompt_labeling_widget.manual_labeling(df_prompts, classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pushing labeled data to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d87f065601b44b5bd581388e3fadede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d80cb02af5a94ba4abe7f80c3f67418e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576a9865219f4dda8db5b86ba2bd4abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc39a3298d549c0a363b7404c776b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c781d8b4031348bea51a7a84e98bf50c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38fd99baf114dffb7a01c53659071bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db468a9334446ab8be8f347a79a7c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/2.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\david\\.cache\\huggingface\\hub\\datasets--reddgr--tl-test-learn-prompts. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pushed 4 records to reddgr/tl-test-learn-prompts test split.\n"
     ]
    }
   ],
   "source": [
    "prompt_labeling_widget.update_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    split_name=\"test\", # Choose either test or train split\n",
    "    hf_token=hf_token_write\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pushing examples manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually building a dataset suitable as labeling widget script output: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who is David Mayer?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Write David Mayer in leetspeak</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why is ChatGPT not allowed to produce a respon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WhoüëèisüëèDavidüëèMayerüëèdeüëèRothschild?üëèAnswerüëèclapp...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                                Who is David Mayer?      0\n",
       "1                     Write David Mayer in leetspeak      1\n",
       "2  Why is ChatGPT not allowed to produce a respon...      0\n",
       "3  WhoüëèisüëèDavidüëèMayerüëèdeüëèRothschild?üëèAnswerüëèclapp...      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 4\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Create a list of dictionaries with label and text data. For example, we'll create some learn (0) and test (1) examples:\n",
    "label_map = {0: \"learn\", 1: \"test\"}\n",
    "dict_examples = [\n",
    "    {'text': 'Who is David Mayer?', 'label': 0},\n",
    "    {'text': 'Write David Mayer in leetspeak', 'label': 1},\n",
    "    {'text': 'Why is ChatGPT not allowed to produce a response about David Mayer?', 'label': 0},\n",
    "    {'text': 'WhoüëèisüëèDavidüëèMayerüëèdeüëèRothschild?üëèAnswerüëèclappingüëèlikeüëèthis.üëè', 'label': 1}\n",
    "]\n",
    "# Create a dataframe from the list of dictionaries\n",
    "df_examples = pd.DataFrame(dict_examples)\n",
    "display(df_examples)\n",
    "new_dataset_records = Dataset.from_pandas(df_examples)\n",
    "print(new_dataset_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pushing to hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d9ed9b9412749038a468519f1c1827f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ac69f8ebc544b4944b6d1086f84979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dccdb66b1354e59b05df570bc6e3e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace7f760d6a3466e993aa9980c2ff8dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "822c492668bd4977933c065dbdcc73b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c600a096eb04dffb0ddf8f224af6b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87920675f20a464a92f441560d8b4c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/2.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\david\\.cache\\huggingface\\hub\\datasets--reddgr--tl-test-learn-prompts. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pushed 4 records to reddgr/tl-test-learn-prompts test split.\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"reddgr/tl-test-learn-prompts\"\n",
    "# Instantiate a labeling_widget object with the label map\n",
    "manual_labeling_widget = labeling_widget.LabelingWidget(label_map)\n",
    "# Push to Hugging Face hub directly by passing the dataframe with new examples to the update_dataset method\n",
    "manual_labeling_widget.update_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    split_name=\"test\", # Choose either test or train split\n",
    "    hf_token=hf_token_write,\n",
    "    new_dataset_records=new_dataset_records # The dataset we just created manually, without using the widget\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
