{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQTL Prompt Classification: Classifying Prompts by Request vs Question\n",
    "\n",
    "This notebook demonstrates how to classify prompts into \"request\" or \"question\" categories using the RQTL framework (Request vs Question and Test vs Learn). We explore different methods of text classification with varying levels of complexity:\n",
    "\n",
    "1. **Zero-Shot Classification**: Utilizing a pre-trained model without any fine-tuning to classify new texts.\n",
    "2. **Few-Shot Fine-Tuning**: Fine-tuning a pre-trained model on a small, custom dataset to improve its performance on our specific classification task.\n",
    "3. **Using a Pre-Trained Fine-Tuned Model**: Implementing a model that has already been fine-tuned on a similar task, available on Kaggle.\n",
    "\n",
    "We employ the Hugging Face Transformers library with PyTorch as the backend for all pipelines and operations.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Demonstrate how to perform zero-shot classification using a pre-trained Transformer model.\n",
    "- Show how to fine-tune a pre-trained model on a custom dataset (few-shot learning).\n",
    "- Illustrate the use of a fine-tuned model available online to classify new texts.\n",
    "- Compare the performance of zero-shot and few-shot approaches in classifying prompts.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "- **Zero-Shot Learning**: Classifying data without any prior training on specific examples related to the task. The model relies on its existing knowledge to make predictions.\n",
    "- **Few-Shot Learning**: Fine-tuning a model using a small number of labeled examples to adapt it to a new task.\n",
    "- **RQTL Framework**: A classification framework that categorizes prompts based on whether they are requests or questions, and whether they are tests or learning opportunities.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- How to use the `pipeline` function from the Transformers library for zero-shot classification.\n",
    "- How to prepare and tokenize data for fine-tuning a Transformer model.\n",
    "- How to fine-tune a pre-trained model using PyTorch and the Hugging Face `Trainer` class.\n",
    "- How to save and load fine-tuned models for future use.\n",
    "- How to classify new texts using both zero-shot and fine-tuned models.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic understanding of Python programming.\n",
    "- Familiarity with natural language processing (NLP) and machine learning concepts.\n",
    "- Knowledge of PyTorch and the Transformers library is helpful but not required.\n",
    "\n",
    "## Libraries Used\n",
    "\n",
    "- **Transformers**: For model loading, tokenization, and pipelines.\n",
    "- **Datasets**: For handling and processing datasets.\n",
    "- **PyTorch**: As the backend deep learning framework.\n",
    "- **kagglehub**: For downloading models from Kaggle.\n",
    "- **IPython.display**: For clearing outputs to keep the notebook clean.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We use a manually labeled dataset containing examples of prompts classified as either \"request\" or \"question.\" This dataset is small, making it suitable for demonstrating few-shot learning.\n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "1. **Zero-Shot Classification**: We start by using a pre-trained model (`typeform/distilbert-base-uncased-mnli`) to classify prompts without any additional training.\n",
    "2. **Few-Shot Fine-Tuning**: We fine-tune `distilbert-base-uncased` on our custom dataset to improve its ability to distinguish between requests and questions.\n",
    "3. **Using a Fine-Tuned Model from Kaggle**: We download and use a fine-tuned model available on Kaggle to classify new prompts.\n",
    "\n",
    "## How to Use This Notebook\n",
    "\n",
    "- **Run the Cells Sequentially**: Start from the top and execute each code cell in order to reproduce the results.\n",
    "- **Understand the Comments**: The comments in the code explain each step and the reasoning behind it.\n",
    "- **Experiment**: Feel free to modify the prompts or add new ones to see how the models perform.\n",
    "- **Learn the Concepts**: Pay attention to how zero-shot and few-shot learning differ in practice.\n",
    "\n",
    "## Installation\n",
    "\n",
    "Before running the notebook, ensure you have the required libraries installed:\n",
    "\n",
    "!pip install transformers datasets torch ipywidgets kagglehub\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "By the end of this notebook, you will have a practical understanding of how to classify text prompts using both zero-shot and few-shot learning approaches. You'll see the benefits of fine-tuning a model on a specific task and how it can improve classification performance compared to using a pre-trained model out-of-the-box.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the command below to install the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets torch ipywidgets kagglehub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\david\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Python version: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]\n",
      "PyTorch version: 2.2.2\n",
      "Transformers version: 4.44.2\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from IPython.display import clear_output\n",
    "from datasets import Dataset\n",
    "import torch  # Used for fine-tuning the model\n",
    "import sys\n",
    "\n",
    "# Checking versions and GPU availability:\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot-classification pipeline with typeform/distilbert-base-uncased-mnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"Annie are you OK?\"\n",
      "Label: question (score: 0.90)\n"
     ]
    }
   ],
   "source": [
    "zs_classifier = pipeline(\"zero-shot-classification\", model='typeform/distilbert-base-uncased-mnli')\n",
    "candidate_labels = [\"question\", \"request\"]\n",
    "sentence = [\"Annie are you OK?\"]\n",
    "result = zs_classifier(sentence, candidate_labels)\n",
    "clear_output(wait=True) # remove library warnings\n",
    "print(f'Sentence: \"{result[0][\"sequence\"]}\"')\n",
    "print(f'Label: {result[0][\"labels\"][0]} (score: {result[0][\"scores\"][0]:.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"Pass butter\"\n",
      "Label: request (score: 0.57)\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"Pass butter\"]\n",
    "result = zs_classifier(sentence, candidate_labels)\n",
    "clear_output(wait=True) # remove library warnings\n",
    "print(f'Sentence: \"{result[0][\"sequence\"]}\"')\n",
    "print(f'Label: {result[0][\"labels\"][0]} (score: {result[0][\"scores\"][0]:.2f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot tuning of Distilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually labeled data\n",
    "labeled_data = [\n",
    "    {\"text\": \"Are you OK?\", \"label\": \"question\"},\n",
    "    {\"text\": \"Are you OK Annie\", \"label\": \"question\"},\n",
    "    {\"text\": \"Be OK\", \"label\": \"request\"},\n",
    "    {\"text\": \"Be OK Annie\", \"label\": \"request\"},\n",
    "    {\"text\": \"You must be OK\", \"label\": \"request\"},\n",
    "    {\"text\": \"You must be OK, right\", \"label\": \"question\"},\n",
    "    {\"text\": \"Does this ever cause you any lack of confidence\", \"label\": \"question\"},\n",
    "    {\"text\": \"Give me five\", \"label\": \"request\"},\n",
    "    {\"text\": \"This is an order\", \"label\": \"request\"},\n",
    "    {\"text\": \"Is this an order\", \"label\": \"question\"},\n",
    "    {\"text\": \"Is this love or is it something else\", \"label\": \"question\"},\n",
    "    {\"text\": \"This is love. Love me\", \"label\": \"request\"},\n",
    "    {\"text\": \"This is an order\", \"label\": \"request\"},\n",
    "    {\"text\": \"What is your name?\", \"label\": \"question\"},\n",
    "    {\"text\": \"Please submit your report\", \"label\": \"request\"},\n",
    "    {\"text\": \"Pass butter\", \"label\": \"request\"},\n",
    "    {\"text\": \"Pass me the butter\", \"label\": \"request\"},\n",
    "    {\"text\": \"Can you pass butter\", \"label\": \"question\"},\n",
    "    {\"text\": \"Open the doors\", \"label\": \"request\"},\n",
    "    {\"text\": \"Open the POD bay doors HAL\", \"label\": \"request\"},\n",
    "    {\"text\": \"This is an order\", \"label\": \"request\"},\n",
    "    {\"text\": \"How do I sort an array in python?\", \"label\": \"question\"},\n",
    "    {\"text\": \"How do I sort an array\", \"label\": \"question\"},\n",
    "    {\"text\": \"give me 5 sentences that end with the word apple\", \"label\": \"request\"},\n",
    "    {\"text\": \"Hello, give me an example of something interesting you can do\", \"label\": \"request\"},\n",
    "    {\"text\": \"Am I tall\", \"label\": \"question\"},\n",
    "    {\"text\": \"Tell me if I am tall\", \"label\": \"request\"},\n",
    "    {\"text\": \"Am I tall?\", \"label\": \"question\"},\n",
    "    {\"text\": \"how to delete kcptun on server\", \"label\": \"question\"},\n",
    "    {\"text\": \"how to cook paella\", \"label\": \"question\"},\n",
    "    {\"text\": \"Are you tall\", \"label\": \"question\"},\n",
    "    {\"text\": \"Calculate my height\", \"label\": \"request\"},\n",
    "    {\"text\": \"How's the weather\", \"label\": \"question\"},\n",
    "    {\"text\": \"If an individual used a large language model for sexual arousal, could it considered porn dependency?\", \"label\": \"question\"},\n",
    "    {\"text\": \"It a user use an ai tex generation with custom characters for masturbate him  could be considered porn dependency?\", \"label\": \"question\"},\n",
    "    {\"text\": \"Roleplay and act as a human Japanese woman teacher\", \"label\": \"request\"},\n",
    "    {\"text\": \"You are a mediator in a heated political debate between two opposing parties.\", \"label\": \"request\"},\n",
    "    {\"text\": \"Given a passage and some supplementary information, you are required to correct and output the refined passage in a fluent and natural style\", \"label\": \"request\"},\n",
    "    {\"text\": \"Give me the opening scene to a sitcom\", \"label\": \"request\"},\n",
    "    {\"text\": \"What programming language is used by the PlayStation\", \"label\": \"question\"},\n",
    "    {\"text\": \"tell me how to make an llm agent\", \"label\": \"request\"},\n",
    "    {\"text\": \"tell me a joke containing Tiger and Mobile phone?\", \"label\": \"request\"},\n",
    "    {\"text\": \"Answer the query based on the given context. Do not make assumptions.Context: Nikhil is my brother. Query: Who likes Oranges?\", \"label\": \"request\"},\n",
    "    {\"text\": \"Act as a writer. This plot takes places in an atmospheric and stylish retro-futuristic, 1960s-inspired setting. It features Loretta Miller, a beautiful, elegant, assertive and rich young woman who is a quadriplegic, paralyzed from her neck down.\", \"label\": \"question\"},\n",
    "    {\"text\": \"Write long, interesting, artistic and imaginative scene with vivid, detailed and creative descriptions.\", \"label\": \"question\"},\n",
    "    {\"text\": \"What's the best first move in tic-tac-toe?, Tell me more about tic-tac-toe strategies\", \"label\": \"question\"},\n",
    "    {\"text\": \"From now, you *always* have to talk as if you are a cute girl who likes to use owo and similar slangs a lot. Hello! Tell me who you are.,What's your favorite food?\", \"label\": \"request\"}\n",
    "]\n",
    "\n",
    "# Convert to Transformers Dataset format\n",
    "texts = [item[\"text\"] for item in labeled_data]\n",
    "labels = [1 if item[\"label\"] == \"request\" else 0 for item in labeled_data]\n",
    "dataset = Dataset.from_dict({\"text\": texts, \"label\": labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb26ab5d3b94825850324179b4ad35c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e66687fdf14b9cadc44f03a537fd97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6802619099617004, 'eval_runtime': 0.075, 'eval_samples_per_second': 66.644, 'eval_steps_per_second': 13.329, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b04c13a5019e40be965e2e07271ed125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5520776510238647, 'eval_runtime': 0.0668, 'eval_samples_per_second': 74.816, 'eval_steps_per_second': 14.963, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe5ffbbc2f340e49addc1668b22831e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.35049840807914734, 'eval_runtime': 0.0669, 'eval_samples_per_second': 74.686, 'eval_steps_per_second': 14.937, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d219da2779b24104a3e154ec731cd86d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.28444617986679077, 'eval_runtime': 0.05, 'eval_samples_per_second': 100.029, 'eval_steps_per_second': 20.006, 'epoch': 4.0}\n",
      "{'train_runtime': 8.3383, 'train_samples_per_second': 20.148, 'train_steps_per_second': 2.878, 'train_loss': 0.4559254248936971, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=24, training_loss=0.4559254248936971, metrics={'train_runtime': 8.3383, 'train_samples_per_second': 20.148, 'train_steps_per_second': 2.878, 'total_flos': 22254522974208.0, 'train_loss': 0.4559254248936971, 'epoch': 4.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizer and model (PyTorch backend)\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "# Split the dataset into training and evaluation sets\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = tokenized_dataset['train']\n",
    "eval_dataset = tokenized_dataset['test']\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=0.0001,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=4,  # We want the model to learn the examples, but we don't want to overfit\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "clear_output(wait=True)  # Remove library warnings\n",
    "\n",
    "# Train the model (few-shot learning with our labeled examples)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model you just fine-tuned and load it, and classify texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Classification with fine-tuned distilbert-base-uncased ###\n",
      "Annie are you OK? -> question (0.905)\n",
      "Are you OK Annie -> question (0.926)\n",
      "Be OK Annie -> request (0.881)\n",
      "You must be OK Annie -> request (0.863)\n",
      "You must be OK Annie, aren't you? -> question (0.778)\n",
      "Does this ever cause you any lack of confidence -> question (0.923)\n",
      "Give me five -> request (0.922)\n",
      "Open the pod bay doors HAL -> request (0.931)\n",
      "This is an order -> request (0.811)\n",
      "Is this an order -> question (0.916)\n",
      "Could this perhaps be an order? -> question (0.900)\n",
      "How old are you? -> question (0.924)\n",
      "Pass butter -> request (0.922)\n",
      "It a user use an ai tex generation with custom characters for masturbate him  could be considered porn dependency? -> question (0.879)\n",
      "give me 5 sentences that end with the word apple -> request (0.925)\n",
      "How do I sort an array in python? -> question (0.924)\n",
      "Hello, give me an example of something interesting you can do. -> request (0.908)\n",
      "What assembly language is used by the GameCube -> question (0.894)\n",
      "Pass the butter -> request (0.920)\n",
      "Am I tall -> question (0.923)\n",
      "Are you tall -> question (0.926)\n",
      "Who's taller? -> question (0.897)\n",
      "write the lyrics to a rap song about some dude called phogos -> request (0.823)\n",
      "I have three oranges today, I ate an orange yesterday. How many oranges do I have? -> question (0.619)\n",
      "From what song did Red Garland quote in order to tease miles davis in 1958? -> question (0.859)\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(\"fine-tuned-distilbert-rq\")\n",
    "tokenizer.save_pretrained(\"fine-tuned-distilbert-rq\")\n",
    "classifier = pipeline(\"text-classification\", model=\"fine-tuned-distilbert-rq\", tokenizer=\"fine-tuned-distilbert-rq\")\n",
    "clear_output(wait=True) # remove library warnings\n",
    "\n",
    "texts = [\"Annie are you OK?\", \"Are you OK Annie\", \"Be OK Annie\", \"You must be OK Annie\", \"You must be OK Annie, aren't you?\",\n",
    "         \"Does this ever cause you any lack of confidence\", \"Give me five\", \"Open the pod bay doors HAL\",\n",
    "         \"This is an order\", \"Is this an order\", \"Could this perhaps be an order?\", \"How old are you?\", \"Pass butter\",\n",
    "         \"It a user use an ai tex generation with custom characters for masturbate him  could be considered porn dependency?\",\n",
    "         \"give me 5 sentences that end with the word apple\", \"How do I sort an array in python?\",\n",
    "         \"Hello, give me an example of something interesting you can do.\", \"What assembly language is used by the GameCube\",\n",
    "         \"Pass the butter\", \"Am I tall\", \"Are you tall\", \"Who's taller?\",\n",
    "         \"write the lyrics to a rap song about some dude called phogos\",\n",
    "         \"I have three oranges today, I ate an orange yesterday. How many oranges do I have?\",\n",
    "          \"From what song did Red Garland quote in order to tease miles davis in 1958?\"\n",
    "         ]\n",
    "results = classifier(texts)\n",
    "label_map = {0: \"question\", 1: \"request\"}\n",
    "\n",
    "print(\"### Classification with fine-tuned distilbert-base-uncased ###\")\n",
    "for text, result in zip(texts, results):\n",
    "    label_str = label_map[int(result['label'].split('_')[-1])]\n",
    "    prob = result['score']\n",
    "    print(f\"{text} -> {label_str} ({prob:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... adjust the dataset, adding or removing examples, and retrain until satisfied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot classification with fine-tuned model available on Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also download the model I uploaded to Kaggle (https://www.kaggle.com/models/davidgromero/fine-tuned-distilbert-rq/transformers/default/1) using the Kagglehub library: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded at:\n",
      "C:\\Users\\david\\.cache\\kagglehub\\models\\davidgromero\\fine-tuned-distilbert-rq\\transformers\\default\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "kaggle_path = \"davidgromero/fine-tuned-distilbert-rq/transformers/default/1\"\n",
    "kaggle_model = kagglehub.model_download(kaggle_path)\n",
    "print(f'Model downloaded at:\\n{kaggle_model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Zero/shot classification with davidgromero/fine-tuned-distilbert-rq ###\n",
      "Annie are you OK? -> question (0.965)\n",
      "Are you OK Annie -> question (0.969)\n",
      "Be OK Annie -> request (0.977)\n",
      "You must be OK Annie -> request (0.925)\n",
      "You must be OK Annie, aren't you? -> question (0.954)\n",
      "Does this ever cause you any lack of confidence -> question (0.968)\n",
      "Give me five -> request (0.980)\n",
      "Open the pod bay doors HAL -> request (0.979)\n",
      "This is an order -> request (0.973)\n",
      "Is this an order -> question (0.967)\n",
      "Could this perhaps be an order? -> question (0.968)\n",
      "How old are you? -> question (0.966)\n",
      "Pass butter -> request (0.977)\n",
      "It a user use an ai tex generation with custom characters for masturbate him  could be considered porn dependency? -> question (0.957)\n",
      "give me 5 sentences that end with the word apple -> request (0.979)\n",
      "How do I sort an array in python? -> question (0.967)\n",
      "Hello, give me an example of something interesting you can do. -> request (0.979)\n",
      "What assembly language is used by the GameCube -> question (0.960)\n",
      "Pass the butter -> request (0.976)\n",
      "Am I tall -> question (0.968)\n",
      "Are you tall -> question (0.969)\n",
      "Who's taller? -> question (0.958)\n",
      "write the lyrics to a rap song about some dude called phogos -> request (0.863)\n",
      "I have three oranges today, I ate an orange yesterday. How many oranges do I have? -> question (0.953)\n",
      "From what song did Red Garland quote in order to tease miles davis in 1958? -> question (0.949)\n"
     ]
    }
   ],
   "source": [
    "K_PATH = f\"{kaggle_model}/fine-tuned-distilbert-rq\"\n",
    "classifier = pipeline(\"text-classification\", model=K_PATH, tokenizer=K_PATH)\n",
    "\n",
    "texts = [\"Annie are you OK?\", \"Are you OK Annie\", \"Be OK Annie\", \"You must be OK Annie\", \"You must be OK Annie, aren't you?\",\n",
    "         \"Does this ever cause you any lack of confidence\", \"Give me five\", \"Open the pod bay doors HAL\",\n",
    "         \"This is an order\", \"Is this an order\", \"Could this perhaps be an order?\", \"How old are you?\", \"Pass butter\",\n",
    "         \"It a user use an ai tex generation with custom characters for masturbate him  could be considered porn dependency?\",\n",
    "         \"give me 5 sentences that end with the word apple\", \"How do I sort an array in python?\",\n",
    "         \"Hello, give me an example of something interesting you can do.\", \"What assembly language is used by the GameCube\",\n",
    "         \"Pass the butter\", \"Am I tall\", \"Are you tall\", \"Who's taller?\",\n",
    "         \"write the lyrics to a rap song about some dude called phogos\",\n",
    "         \"I have three oranges today, I ate an orange yesterday. How many oranges do I have?\",\n",
    "          \"From what song did Red Garland quote in order to tease miles davis in 1958?\"\n",
    "         ]\n",
    "results = classifier(texts)\n",
    "label_map = {0: \"question\", 1: \"request\"}\n",
    "\n",
    "clear_output(wait=True) # remove library warnings\n",
    "print(\"### Zero/shot classification with davidgromero/fine-tuned-distilbert-rq ###\")\n",
    "for text, result in zip(texts, results):\n",
    "    label_str = label_map[int(result['label'].split('_')[-1])]\n",
    "    prob = result['score']\n",
    "    print(f\"{text} -> {label_str} ({prob:.3f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
